---
title: "Assign3"
execute:
  eval: true
format:
  html:
    toc: true
    toc-depth: 3
---

# Setup

Some packages:

```{r}
#| label: packages
#| collapse: TRUE
library(dplyr)
library(ggplot2)
library(viridis)
library(patchwork)
set_theme(theme_bw()) # for ggplot formatting
```

Now the data:

```{r}
#| label: load-data
#| collapse: TRUE
anpp <- read.csv('global_ANPP_data.csv', header = TRUE) |>
  select(-X) # get rid of "X" column that's an artifact of the file
str(anpp)
```

AF only has 3 observations; if we include region as a factor, we should exclude these data because there just aren't enough to make robust inference.

# EDA: variable distributions -- Q1 and 2

## Histograms

A histogram plotting function to avoid a ton of redundant code:

```{r}
#| label: plot-hist
# We use {{var}} so we can use dplyr-style data masking: from Hadley Wickham
#> "programming with dplyr" page

plot_hist <- function(data, 
                      var,
                      color = 'grey20',
                      fill = 'lightgrey',
                      fill_alpha = 0.75,
                      num_bins = 10,
                      add_mean = TRUE,
                      line_col = 'red',
                      line_type = 'dashed'){
  p <- ggplot(data)+
    geom_histogram(aes(x = {{var}}),
                   color = color,
                   fill = fill,
                   bins = num_bins)+
      labs(y = 'Count')
  

  if(add_mean == TRUE){
    p <- p+geom_vline(aes(xintercept = mean({{var}})),
                      color = line_col,
                      linetype = line_type)
  }   
  
  return(p)
}
```

And now let's use the code to make some histograms.

```{r}
#| label: histograms
#| 

# MAT
mat_hist <- plot_hist(data = anpp,
                      var = MAT)

# MAP
map_hist <- plot_hist(data = anpp,
                      var = MAP)

logmap_hist <- plot_hist(data = anpp,
                         var = log(MAP))

# ANPP
anpp_hist <- plot_hist(data = anpp,
                       var = ANPP)

# Now make a figure:
## Overall
mat_hist+map_hist+anpp_hist+plot_layout(axis_titles = 'collect')+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'),
        axis.text = element_text(size = 8))
```

## Scatterplots

Again, a function to save typing. Since we're going to be using a gamma distribution with a log link, let's examine linearity of predictors WRT log(ANPP) (though this behavior is customizable with the `yvar` option).

```{r}
plot_scatter <- function(data,
                         xvar,
                         yvar = log(ANPP),
                         pt_alpha = 0.5,
                         smooths = TRUE,
                         smooth_col = 'blue',
                         smooth_lty = 'dashed',
                         smooth_lty_rgn = 'solid'){
  
  # plot template
  p <- ggplot(data, aes(x = {{xvar}}, y = {{yvar}}))+
    geom_point(alpha = pt_alpha)
  
  if(smooths == TRUE){
    p <- p+geom_smooth(formula = y ~ x,
                method = 'loess',
                se = FALSE,
                col = smooth_col,
                linetype = smooth_lty)
  }
  return(p)
}
```

Using that function:

```{r}
#| label: scatterplots

# MAT
mat_scatter <- plot_scatter(anpp, xvar = MAT, yvar = ANPP, smooth_lty = 'solid')


# MAP
map_scatter <- plot_scatter(anpp, xvar = MAP, yvar = ANPP, smooth_lty = 'solid')



# Log MAP
log_map_scatter <- plot_scatter(anpp, xvar = log(MAP), smooth_lty = 'solid')


# Figures
## Overall
mat_scatter+map_scatter+plot_layout(axis_titles = 'collect') 

## Log transformed
mat_scatter+log_map_scatter+plot_layout(axis_titles = 'collect')+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))

```

## Takeaways (Q3)

We can see a few things here:

1.  ANPP is positive, continuous, and skewed (from the histogram)
2.  ANPP variance is not constant with respect to MAT (from the scatterplot): higher MAT has higher variance (heteroskedacity)
3.  ANPP is not linearly related to MAP.
4.  MAP is not normally distributed; it is strictly positive and right-skewed. log(MAP) is much closer to normal.
5.  MAT is closer to normally distributed, but it still has a sort of weird shape.
6.  We recover a more linear relationship between log(MAP) and ANPP, but variance is still non-constant.

Taking 1 and 2 together, we should use either a Gamma or Inverse Gaussian distribution as the response distribution. We'll use Gamma because our data are not so incredibly skewed.

# Modeling

## Full Model

As always, we'll start by running a full model.

### Running the model

```{r}
m1 <- glm(ANPP ~ MAT*MAP,
          family = Gamma(link = 'log'),
          data = anpp)
```

### Diagnostics

```{r}
plot(m1)
```

We have some higher leverage points (but still within acceptable Cook's distance); let's see where these points fall on the distribution of MAT and MAP:

```{r}
#| label: leverage-points

m1_lev <- c(37, 38, 46)
m1_mat_lev <- anpp[m1_lev, 3]
m1_map_lev <- anpp[m1_lev, 4]

hist(anpp$MAT)
abline(v = m1_mat_lev, col = 'red')

hist(anpp$MAP)
abline(v = m1_map_lev, col = 'red')
```

We can see that, in fact, the high leverage points for MAP are on the tail end. So then we can log-transform the MAP data to get it better behaved:

```{r}
#| label: examine-leverage
hist(log(anpp$MAP))
abline(v = log(m1_map_lev), col = 'red')
```

While these points still fall on the tails, they may have less leverage now. Let's check:

```{r}
#| label: full-model-log
m1_log <- glm(ANPP ~ MAT*log(MAP),
                family = Gamma(link = 'log'),
                data = anpp)
plot(m1_log)
```

Leverage plot looks a lot better, but residuals are a bit wonky at the tail end of the CDF (Q-Q plot). Let's see if this model fits better than the un-logged one, using AIC:

```{r}
AIC(m1, m1_log)
```

Looks like, as we suspected from residuals, that m1_log is better.

We'll make a figure comparing the residuals vs fitted and cook's distance for both models.

```{r}

```

### ANOVA

Let's now look at an analysis of deviance to see if we can pare this model down.

```{r}
anova(m1_log, test = 'Chisq')
```

It looks like no: both terms and their interactions contribute significantly to decreasing the residual deviance. Because these models are nested, we don't even need to use AIC. But just to check, let's run a model that doesn't include the interaction term.

## Simplified Model 2

### Running the model

Let's remove the interaction term. If all is right in the world, our ANOVA and AIC results should agree.

```{r}
m2_log <- glm(ANPP ~ log(MAP)+MAT,
              family = Gamma(link = 'log'),
              data = anpp)
```

### Diagnostics

```{r}
plot(m2_log)
```

## Simplified model 3 and 4

This will be MAT only:

```{r}
m3_log <- glm(ANPP ~ MAT,
              family = Gamma(link = 'log'),
              data = anpp)
```

And MAP only:

```{r}
m4_log <- glm(ANPP ~ log(MAP),
              family = Gamma(link = 'log'),
              data = anpp)
```

Let's use AIC to compare this model to the full model. If all is right in the world, this will have a higher AIC (agreeing with the ANOVA results):

```{r}
AIC(m1_log, m2_log, m3_log, m4_log, m1) |>
  arrange(AIC)
```

These results are as expected.

# Model Assessment

Now that we have selected our best-fitting model, let's look at the coefficients:

```{r}
m1_summ <- summary(m1_log)
m1_summ
```

We can then calculate $D^2$, analogous to $R^2$:

```{r}
d2 = 1-exp(-(m1_summ$null.deviance-m1_summ$deviance)/m1_summ$deviance)
```

Plotting partial residuals:

```{r}
par(mfrow = c(1,2))
termplot(m1_log, data = anpp, partial.resid = TRUE)
```

Plotting hat values:

```{r}
plot(anpp$MAT, hatvalues(m1_log))
plot(log(anpp$MAP), hatvalues(m1_log))
```

Both predictors have higher leverage points at the extremes, which is somewhat concerning.

Looking at observed vs predicted values:

```{r}
preds <- predict(m1_log, data = anpp)
plot(log(anpp$ANPP), preds)
```

## Transforming coefficients

First, some R setup:

```{r}
b0 <- m1_log$coefficients[1] # intercept
b1 <- m1_log$coefficients[2] # MAT
b2 <- m1_log$coefficients[3] # log(MAP)
b3 <- m1_log$coefficients[4] # interaction
```

Now some math. Our model formulation is:

$$
\log(\text{ANPP}) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log(\text{MAP})+\beta_3 \text{MAT}\log(\text{MAP})
$$

### MAT coefficient

Consider a 1-unit increase in MAT when log(MAP) is 0. Note $\beta_0 + \beta_2 \log(\text{MAP})+\beta_3 \text{MAT}\log(\text{MAP}) = \beta_0$.

$$
\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT}\\
\log(\text{ANPP}_2) = \beta_0 + \beta_1 (\text{MAT}+1)
$$

```{=latex}
\begin{align*}
\implies \log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_1 \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_1 \\
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_1}
\end{align*}
```

Calculating $e^{\beta_1}$:

```{r}
mat_coeff <- exp(b1)
mat_coeff
```

So a 1-unit increase in MAT means a 1.135x increase in ANPP, AKA a 14% increase.

### MAP coefficient

Consider a 10% increase in MAP when MAT is 0. Because MAT = 0, $\beta_0 + \beta_1 \text{MAT} +\beta_3 \text{MAT}\log(\text{MAP}) =  \beta_0$.

$$
\log(\text{ANPP}_1) = \beta_0 + \beta_2\log(\text{MAP}) \\
\log(\text{ANPP}_2) = \beta_0 + \beta_2\log(\text{1.1MAP})
$$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_2(\log(1.1\text{MAP})-\log(\text{MAP})) \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_2(\log(\frac{1.1\text{MAP}}{\text{MAP}})) \\
&= \beta_2 \log(1.1)
\end{align*} 

$\implies \frac{\text{ANPP}_2}{\text{ANPP}_1} = e^{\beta_2 \log(1.1)}$
```

Calculating $e^{\beta_2 \log(1.1)}$:

```{r}
map_coeff <- exp(b2*log(1.1))
map_coeff
```

So a 10% increase in MAP means a 1.03x increase in ANPP, AKA a 3% increase.

### Interaction

This one is the most involved.

#### MAT constant

Let's first consider the case where MAT is held constant. In this case, we're finding the contribution of MAP conditional on a value of MAT. Then:

$\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{MAP} + \beta_3 \text{MAT}\log(\text{MAP})$

$\log(\text{ANPP}_2) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{1.1MAP} + \beta_3 \text{MAT}\log(\text{1.1MAP})$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_2(\log(1.1\text{MAP})-\log(\text{MAP})) + \beta_3(\text{MAT}\log(\text{1.1MAP})-\text{MAT}\log(\text{MAP})) \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_2 \log(1.1) + \beta_3 \text{MAT}\log(1.1) 
\end{align*}
```

```{=latex}
\begin{align*}
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_2 \log(1.1) + \beta_3 \text{MAT}\log(1.1)} \\
&= e^{\beta_2\log(1.1)}*e^{\beta_3\log(1.1)\text{MAT}} \\
&= e^{\beta_2\log(1.1)}*\exp({\beta_3\log(1.1)})^{\text{MAT}}
\end{align*}
```

The first part of this is identical to the effect of a 10% increase in MAP on ANPP. Let's calculate the second part:

```{r}
map_base <- exp(b3*log(1.1))
map_base
```

#### log(MAP) constant

Let's now consider the other case, with log(MAP) held constant:

$\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{MAP} + \beta_3 \text{MAT}\log(\text{MAP})$

$\log(\text{ANPP}_2) = \beta_0 + \beta_1 \text{MAT+1} + \beta_2 \log\text{MAP} + \beta_3 \text{(MAT+1)}\log(\text{MAP})$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_1(\text{MAT}+1-\text{MAT}) + \beta_3 ((\text{MAT+1})\log(\text{MAP})- \text{MAT}\log(\text{MAP})) \\
&= \beta_1 + \beta_3\log(\text{MAP}) 
\end{align*}
```

```{=latex}
\begin{align*}
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_1 + \beta_3 \log\text{MAP}} \\
&= e^{\beta_1}*e^{\beta_3\log\text{MAP}} \\
&= e^{\beta_1}*(e^{\log\text{MAP}})^{\beta_3} \\
&= e^{\beta_1}\text{MAP}^{\beta_3}
\end{align*}
```

Similarly, note that the first term here is identical to the transformed coefficient for MAT.

### Plotting marginal effects

```{r}
mat_vals <- seq(min(anpp$MAT), max(anpp$MAT), length.out = 100)
map_vals <- seq(min(anpp$MAP), max(anpp$MAP), length.out = 100)
marg_df <- data.frame(MAP = map_vals,
                      mat_marginal = mat_coeff*map_vals^b3,
                      MAT = mat_vals,
                      map_marginal = map_coeff*map_base^mat_vals)
mat_marg <- ggplot(data = marg_df,
                   aes(x = MAP, y = 100*(mat_marginal-1)))+
  geom_line()+
  geom_hline(aes(yintercept = 0),
             color = 'red',
             linetype = 'dashed')+
  labs(x = 'MAP Value', y = '% change ANPP/increase MAT')

map_marg <- ggplot(data = marg_df,
                   aes(x = MAT, y = 100*(map_marginal-1)))+
  geom_line()+
  geom_hline(aes(yintercept = 0),
             color = 'red',
             linetype = 'dashed')+
  labs(x = 'MAT Value', y = '% change ANPP/ 10% increase MAP')
mat_marg
```

Let's now plot the surface of ANPP by MAT and MAP:

```{r}
mat_df <- marg_df |>
  select(MAT, MAP) |>
  tidyr::complete(MAT, MAP)

mat_df$ANPP <- predict(m1_log, newdata = mat_df, type = 'response')

composite <- ggplot(mat_df, aes(x = MAP, y = MAT, z = ANPP))+
  geom_contour_filled(bins = 15)+
  scale_fill_viridis_d()+
  theme(panel.border = element_rect(color = NA, fill = NA),
        legend.position = 'bottom')+
  labs(fill = 'ANPP')

composite
```

# Accuracy and Validation 

## Model Validation: 10-Fold CV for best fitting model

A cross-validation function:
```{r}
cross_valid_continuous <- function(model, data, response_col, k = 10, seed = NULL) {
  # set the seed if requested, for repeatable results
  if(!is.null(seed)){
    set.seed(seed)
  }
  
  # check for disagreement
  stopifnot(response_col %in% names(data))

  # assign folds
  N <- nrow(data)
  fold <- sample(rep(1:k), N, replace = TRUE)
  x <- cbind(data, fold)
  
  # initialize storage vectors
  ME   <- numeric(k)
  MAE  <- numeric(k)
  RMSE <- numeric(k)

  for (i in seq_len(k)) {
    train_data <- subset(x, fold != i)
    valid_data <- subset(x, fold == i)
    
    # check that validation set is actually good
    if (nrow(valid_data) == 0){ 
      warning(paste('No validation data for fold', i))
      ME[i] <- NA
      MAE[i] <- NA
      RMSE[i] <- NA
    }else{
      # Refit on training fold
      fold_model <- update(model, data = train_data)
  
      # Predict on validation fold (response scale)
      preds <- predict(fold_model, newdata = valid_data, type = "response")
      y     <- valid_data[[response_col]]
  
      ME[i]   <- mean(preds - y)
      MAE[i]  <- mean(abs(preds - y))
      RMSE[i] <- sqrt(mean((preds - y)^2))
    }
  }

  out <- data.frame(fold = 1:k, ME = ME, MAE = MAE, RMSE = RMSE)
  attr(out, "summary") <- with(out, c(
    ME_mean   = mean(ME,   na.rm = TRUE), ME_sd   = sd(ME,   na.rm = TRUE),
    MAE_mean  = mean(MAE,  na.rm = TRUE), MAE_sd  = sd(MAE,  na.rm = TRUE),
    RMSE_mean = mean(RMSE, na.rm = TRUE), RMSE_sd = sd(RMSE, na.rm = TRUE)
  ))
  return(out)
}
```

```{r}
# Your fitted model
# m1_log <- glm(ANPP ~ MAT*log(MAP), family = Gamma(link = "log"), data = anpp)

cv_m1 <- cross_valid_continuous(model = m1_log,
                                data  = anpp,    
                                response_col = "ANPP",  
                                k = 10,
                                seed = 1)

summary(cv_m1)              # fold-by-fold summary
attr(cv_m1, "summary")      # overall ME/MAE/RMSE means & SDs


cv_plot <- cv_m1 |>
           tidyr::pivot_longer(cols = c(ME, MAE, RMSE),
                               names_to = 'metric',
                               values_to = 'value') |> # put in ggplot friendly format
            ggplot(aes(group = metric, x = metric, y = value))+
            geom_boxplot(outliers = FALSE)+
            geom_jitter(alpha = 0.5, size = 2, width = 0.05)+
            labs(x = 'Accuracy Metric',
                 y = 'Estimate')
  

cv_plot
```
## Linear regression for bias

A second regression of predicted vs. observed ANPP values evaluates model bias and accuracy:
**Intercept:** 6.63 shows our model over predicts ANPP even when ANPP=0 the model predicts about 6.6 
**Slope:** Every 1-unit increase in observed ANPP our model prediction only increases 0.3 units 
**R2:** Only 29.36% of the variation in ANPP is explained by MAT*log(MAP) which is not very strong

This model over predicts at low ANPP and under predicts at high ANPP suggesting limited predictive ability  
```{r}
n <- nrow(anpp)
samp <- sample(2,n,replace = TRUE)
data.valid <- cbind(samp,anpp)
data.train <- subset(anpp,samp == 1)
data.valid <- subset(anpp,samp == 2)

train.glm <- glm(ANPP ~ MAT * log(MAP),
                family = Gamma(link = "log"),
                data = data.train)

summary(train.glm)
# predict on held-out data

pred_valid <- predict(train.glm, newdata = data.valid, type = 'response')

```
## Observed vs predicted 

We now run a linear regression on predictions generated from the test set.
```{r}
lm.valid <- lm(pred_valid ~ data.valid$ANPP)
summary(lm.valid)

pal <- turbo(2, begin = 0.25, end = 0.75) # colorblind distinguishable colors

lm_valid_plot <- ggplot(data.frame(Observed = data.valid$ANPP,
                                   Predicted = pred_valid))+
  geom_point(aes(x = Observed, y = Predicted), alpha = 0.5)+
  geom_abline(slope = lm.valid$coefficients[2],
              intercept = lm.valid$coefficients[1],
              color = pal[1])+
  geom_abline(slope = 1,
              intercept = 0,
              color = pal[2],
              lty = 'dashed')+
  coord_fixed()+
  scale_x_continuous(limits = c(0, 27))+
  scale_y_continuous(limits = c(0, 27))+
  annotate(geom = 'text',
           x = 5, y = 25, label = 'bold(R^2 == 0.25)', parse = TRUE)
  

lm_valid_plot
```

# Figures

## Figure 1

Let's construct figure 1. The left panel (a) will be the ANPP histogram. The right panels (b, c) will be the scatterplots.

```{r}
fig1 <- anpp_hist+mat_scatter/map_scatter+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig1
```

## Figure 2

The histograms for the predictors, side-by-side.

```{r}
fig2 <- mat_hist+map_hist+logmap_hist+
  plot_layout(axes = 'collect')+
  plot_annotation(tag_levels = 'a',
                  tag_prefix = '(',
                  tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig2
```

## Figure 4

These will be the residuals diagnostics plots.

```{r}
rd_df <- data.frame(predicted_values = predict(m1_log, anpp),
                    residuals = residuals(m1_log, type = 'pearson'),
                    stand_dev_residuals = rstandard(m1_log, type = 'deviance'),
                    stand_pear_residuals = rstandard(m1_log, type = 'pearson'),
                    hat_values = hatvalues(m1_log))

# get theoretical quantiles that match what base R uses
rd_df$theo_quant <- abs(qnorm(ppoints(nrow(rd_df))/2))

resids_fitted <- ggplot(rd_df, aes(x = predicted_values, y = residuals))+
  geom_point(alpha = 0.5)+
  geom_hline(aes(yintercept = 0), col = 'grey', lty = 'dashed')+
  geom_smooth(se = FALSE, col = 'red')+
  labs(x = 'Predicted Value',
       y = 'Pearson\'s Residual')

# Q-Q plot
qq <- ggplot(rd_df, aes(x = sort(theo_quant), y = sort(abs(stand_dev_residuals))))+
  geom_point(alpha = 0.5)+
  geom_abline(aes(intercept = 0, slope = 1), col = 'grey', lty = 'dashed')+
  labs(x = 'Theoretical Quantiles', y = '|Std. Deviance Residuals|')

scale_location <- ggplot(rd_df, aes(x = predicted_values, 
                                    y = sqrt(abs(stand_pear_residuals))))+
  geom_point(alpha = 0.5)+
  geom_smooth(se = FALSE, col = 'red')+
  labs(x = 'Predicted Value',
       y = expression(sqrt('|Std. Pearson\'s Residual|')))

resid_leverage <- ggplot(rd_df, aes(x = hat_values,
                                    y = stand_pear_residuals))+
  geom_point(alpha = 0.5)+
  geom_smooth(se = FALSE, col = 'red')+
  geom_hline(aes(yintercept = 0), col = 'grey', lty = 'dashed')+
  labs(x = 'Leverage',
       y = 'Std. Pearson\'s Residual')

fig4 <- (resids_fitted+qq)+
  plot_annotation(tag_levels = 'a',
                  tag_prefix = '(',
                  tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig4
```

## Figure 3

This is a plot of (a) the surface of ANPP by MAP and MAT, (b) the marginal effect of MAP on MAT and (c) the marginal effect of MAT on MAP.

```{r}
# layout <- '
# BBBB#
# AAAAC
# AAAAC
# AAAAC
# AAAAC
# '
# 
# map_rotate <- ggplot(marg_df)+
#   geom_line(aes(x = map_marginal, y = MAT))+
#   scale_x_continuous(n.breaks = 3)+
#   theme(axis.text.x = element_text(angle = 270))+
#   labs(x = 'MAP|MAT')

# fig4 <- composite+scale_x_continuous(breaks = seq(1000, 7000, by = 1000))+
#   mat_marg+scale_x_continuous(breaks = seq(1000, 7000, by = 1000))+
#   theme(axis.title.x = element_blank(),
#                  axis.text.x = element_blank(),
#                  #axis.ticks.y = element_blank(),
#                  panel.border = element_blank(),
#                  panel.grid = element_blank(),
#                  axis.line = element_line(color = 'darkgrey'))+
#   map_rotate+theme(axis.title.y = element_blank(),
#                    axis.text.y = element_blank(),
#                    panel.border = element_blank(),
#                    panel.grid = element_blank(),
#                    axis.line = element_line(color = 'darkgrey'))+
#   plot_layout(design = layout)

layout <- '
AAAB
AAAB
AAAC
AAAC
'
fig3 <- composite + mat_marg+map_marg+
  plot_layout(design = layout)+
  plot_annotation(tag_levels = 'a',
                  tag_prefix = '(',
                  tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
  
                  
fig3
```

