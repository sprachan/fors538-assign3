---
title: "Assign3"
execute:
  eval: true
format:
  html:
    toc: true
    toc-depth: 3
---

# Setup

Some packages:
```{r}
#| label: packages
#| collapse: TRUE
library(dplyr)
library(ggplot2)
library(patchwork)
set_theme(theme_bw()) # for ggplot formatting
```

Now the data:
```{r}
#| label: load-data
#| collapse: TRUE
anpp <- read.csv('global_ANPP_data.csv', header = TRUE) |>
  select(-X) # get rid of "X" column that's an artifact of the file
str(anpp)
```

AF only has 3 observations; if we include region as a factor, we should exclude these data because there just aren't enough to make robust inference.

# EDA: variable distributions -- Q1 and 2

## Histograms

A histogram plotting function to avoid a ton of redundant code:

```{r}
#| label: plot-hist
# We use {{var}} so we can use dplyr-style data masking: from Hadley Wickham
#> "programming with dplyr" page

plot_hist <- function(data, 
                      var,
                      color = 'grey20',
                      fill = 'lightgrey',
                      fill_alpha = 0.75,
                      num_bins = 10,
                      add_mean = TRUE,
                      line_col = 'red',
                      line_type = 'dashed'){
  p <- ggplot(data)+
    geom_histogram(aes(x = {{var}}),
                   color = color,
                   fill = fill,
                   bins = num_bins)+
      labs(y = 'Count')
  

  if(add_mean == TRUE){
    p <- p+geom_vline(aes(xintercept = mean({{var}})),
                      color = line_col,
                      linetype = line_type)
  }   
  
  return(p)
}
```


And now let's use the code to make some histograms.

```{r}
#| label: histograms
#| 

# MAT
mat_hist <- plot_hist(data = anpp,
                      var = MAT)

# MAP
map_hist <- plot_hist(data = anpp,
                      var = MAP)

logmap_hist <- plot_hist(data = anpp,
                         var = log(MAP))

# ANPP
anpp_hist <- plot_hist(data = anpp,
                       var = ANPP)

# Now make a figure:
## Overall
mat_hist+map_hist+anpp_hist+plot_layout(axis_titles = 'collect')+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'),
        axis.text = element_text(size = 8))
```


## Scatterplots

Again, a function to save typing. Since we're going to be using a gamma distribution with a log link, let's examine linearity of predictors WRT log(ANPP) (though this behavior is customizable with the `yvar` option).

```{r}
plot_scatter <- function(data,
                         xvar,
                         yvar = log(ANPP),
                         pt_alpha = 0.5,
                         smooths = TRUE,
                         smooth_col = 'blue',
                         smooth_lty = 'dashed',
                         smooth_lty_rgn = 'solid'){
  
  # plot template
  p <- ggplot(data, aes(x = {{xvar}}, y = {{yvar}}))+
    geom_point(alpha = pt_alpha)
  
  if(smooths == TRUE){
    p <- p+geom_smooth(formula = y ~ x,
                method = 'loess',
                se = FALSE,
                col = smooth_col,
                linetype = smooth_lty)
  }
  return(p)
}
```

Using that function:

```{r}
#| label: scatterplots

# MAT
mat_scatter <- plot_scatter(anpp, xvar = MAT, yvar = ANPP, smooth_lty = 'solid')


# MAP
map_scatter <- plot_scatter(anpp, xvar = MAP, yvar = ANPP, smooth_lty = 'solid')



# Log MAP
log_map_scatter <- plot_scatter(anpp, xvar = log(MAP), smooth_lty = 'solid')


# Figures
## Overall
mat_scatter+map_scatter+plot_layout(axis_titles = 'collect') 

## Log transformed
mat_scatter+log_map_scatter+plot_layout(axis_titles = 'collect')+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))

```



## Takeaways (Q3)

We can see a few things here:

1. ANPP is positive, continuous, and skewed (from the histogram)
2. ANPP variance is not constant with respect to MAT (from the scatterplot): higher MAT has higher variance (heteroskedacity)
3. ANPP is not linearly related to MAP.
4. MAP is not normally distributed; it is strictly positive and right-skewed. log(MAP) is much closer to normal.
5. MAT is closer to normally distributed, but it still has a sort of weird shape.
6. We recover a more linear relationship between log(MAP) and ANPP, but variance is still non-constant.

Taking 1 and 2 together, we should use either a Gamma or Inverse Gaussian distribution as the response distribution. We'll use Gamma because our data are not so incredibly skewed.


# Modeling 

## Full Model

As always, we'll start by running a full model.
 
### Running the model

```{r}
m1 <- glm(ANPP ~ MAT*MAP,
          family = Gamma(link = 'log'),
          data = anpp)
```
### Diagnostics

```{r}
plot(m1)
```

We have some higher leverage points (but still within acceptable Cook's distance); let's see where these points fall on the distribution of MAT and MAP:

```{r}
#| label: leverage-points

m1_lev <- c(37, 38, 46)
m1_mat_lev <- anpp[m1_lev, 3]
m1_map_lev <- anpp[m1_lev, 4]

hist(anpp$MAT)
abline(v = m1_mat_lev, col = 'red')

hist(anpp$MAP)
abline(v = m1_map_lev, col = 'red')
```

We can see that, in fact, the high leverage points for MAP are on the tail end. So then we can log-transform the MAP data to get it better behaved:

```{r}
#| label: examine-leverage
hist(log(anpp$MAP))
abline(v = log(m1_map_lev), col = 'red')
```


While these points still fall on the tails, they may have less leverage now. Let's check:


```{r}
#| label: full-model-log
m1_log <- glm(ANPP ~ MAT*log(MAP),
                family = Gamma(link = 'log'),
                data = anpp)
plot(m1_log)
```

Leverage plot looks a lot better, but residuals are a bit wonky at the tail end of the CDF (Q-Q plot). Let's see if this model fits better than the un-logged one, using AIC:

```{r}
AIC(m1, m1_log)
```

Looks like, as we suspected from residuals, that m1_log is better.

We'll make a figure comparing the residuals vs fitted and cook's distance for both models.

```{r}

```


### ANOVA

Let's now look at an analysis of deviance to see if we can pare this model down.

```{r}
anova(m1_log, test = 'Chisq')
```

It looks like no: both terms and their interactions contribute significantly to decreasing the residual deviance. Because these models are nested, we don't even need to use AIC. But just to check, let's run a model that doesn't include the interaction term.

## Simplified Model 2

### Running the model

Let's remove the interaction term. If all is right in the world, our ANOVA and AIC results should agree.

```{r}
m2_log <- glm(ANPP ~ log(MAP)+MAT,
              family = Gamma(link = 'log'),
              data = anpp)
```

### Diagnostics

```{r}
plot(m2_log)
```

## Simplified model 3 and 4
This will be MAT only:

```{r}
m3_log <- glm(ANPP ~ MAT,
              family = Gamma(link = 'log'),
              data = anpp)
```

And MAP only:

```{r}
m4_log <- glm(ANPP ~ log(MAP),
              family = Gamma(link = 'log'),
              data = anpp)
```

Let's use AIC to compare this model to the full model. If all is right in the world, this will have a higher AIC (agreeing with the ANOVA results):

```{r}
AIC(m1_log, m2_log, m3_log, m4_log, m1) |>
  arrange(AIC)
```

These results are as expected.

# Model Assessment

Now that we have selected our best-fitting model, let's look at the coefficients:

```{r}
m1_summ <- summary(m1_log)
m1_summ
```
We can then calculate $D^2$, analogous to $R^2$:

```{r}
d2 = 1-exp(-(m1_summ$null.deviance-m1_summ$deviance)/m1_summ$deviance)
```

Plotting partial residuals:

```{r}
par(mfrow = c(1,2))
termplot(m1_log, data = anpp, partial.resid = TRUE)
```

Plotting hat values:

```{r}
plot(anpp$MAT, hatvalues(m1_log))
plot(log(anpp$MAP), hatvalues(m1_log))
```

Both predictors have higher leverage points at the extremes, which is somewhat concerning.

Looking at observed vs predicted values:

```{r}
preds <- predict(m1_log, data = anpp)
plot(log(anpp$ANPP), preds)
```


## Transforming coefficients

First, some R setup:
```{r}
b0 <- m1_log$coefficients[1] # intercept
b1 <- m1_log$coefficients[2] # MAT
b2 <- m1_log$coefficients[3] # log(MAP)
b3 <- m1_log$coefficients[4] # interaction
```

Now some math. Our model formulation is:

$$
\log(\text{ANPP}) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log(\text{MAP})+\beta_3 \text{MAT}\log(\text{MAP})
$$

### MAT coefficient

Consider a 1-unit increase in MAT when log(MAP) is 0. Note $\beta_0 + \beta_2 \log(\text{MAP})+\beta_3 \text{MAT}\log(\text{MAP}) = \beta_0$.

$$
\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT}\\
\log(\text{ANPP}_2) = \beta_0 + \beta_1 (\text{MAT}+1)
$$
```{=latex}
\begin{align*}
\implies \log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_1 \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_1 \\
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_1}
\end{align*}
```

Calculating $e^{\beta_1}$:
```{r}
mat_coeff <- exp(b1)
mat_coeff
```
So a 1-unit increase in MAT means a 1.135x increase in ANPP, AKA a 14% increase.

### MAP coefficient

Consider a 10% increase in MAP when MAT is 0. Because MAT = 0, $\beta_0 + \beta_1 \text{MAT} +\beta_3 \text{MAT}\log(\text{MAP}) =  \beta_0$.

$$
\log(\text{ANPP}_1) = \beta_0 + \beta_2\log(\text{MAP}) \\
\log(\text{ANPP}_2) = \beta_0 + \beta_2\log(\text{1.1MAP})
$$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_2(\log(1.1\text{MAP})-\log(\text{MAP})) \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_2(\log(\frac{1.1\text{MAP}}{\text{MAP}})) \\
&= \beta_2 \log(1.1)
\end{align*} 

$\implies \frac{\text{ANPP}_2}{\text{ANPP}_1} = e^{\beta_2 \log(1.1)}$
```

Calculating $e^{\beta_2 \log(1.1)}$:
```{r}
map_coeff <- exp(b2*log(1.1))
map_coeff
```

So a 10% increase in MAP means a 1.03x increase in ANPP, AKA a 3% increase.

### Interaction

This one is the most involved. 

#### MAT constant

Let's first consider the case where MAT is held constant. In this case, we're finding the contribution of MAP conditional on a value of MAT. Then:

$\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{MAP} + \beta_3 \text{MAT}\log(\text{MAP})$

$\log(\text{ANPP}_2) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{1.1MAP} + \beta_3 \text{MAT}\log(\text{1.1MAP})$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_2(\log(1.1\text{MAP})-\log(\text{MAP})) + \beta_3(\text{MAT}\log(\text{1.1MAP})-\text{MAT}\log(\text{MAP})) \\
\log(\frac{\text{ANPP}_2}{\text{ANPP}_1}) &= \beta_2 \log(1.1) + \beta_3 \text{MAT}\log(1.1) 
\end{align*}
```


```{=latex}
\begin{align*}
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_2 \log(1.1) + \beta_3 \text{MAT}\log(1.1)} \\
&= e^{\beta_2\log(1.1)}*e^{\beta_3\log(1.1)\text{MAT}} \\
&= e^{\beta_2\log(1.1)}*\exp({\beta_3\log(1.1)})^{\text{MAT}}
\end{align*}
```

The first part of this is identical to the effect of a 10% increase in MAP on ANPP. Let's calculate the second part:

```{r}
map_base <- exp(b3*log(1.1))
map_base
```


#### log(MAP) constant

Let's now consider the other case, with log(MAP) held constant:

$\log(\text{ANPP}_1) = \beta_0 + \beta_1 \text{MAT} + \beta_2 \log\text{MAP} + \beta_3 \text{MAT}\log(\text{MAP})$

$\log(\text{ANPP}_2) = \beta_0 + \beta_1 \text{MAT+1} + \beta_2 \log\text{MAP} + \beta_3 \text{(MAT+1)}\log(\text{MAP})$

```{=latex}
\begin{align*}
\log(\text{ANPP}_2)-\log(\text{ANPP}_1) &= \beta_1(\text{MAT}+1-\text{MAT}) + \beta_3 ((\text{MAT+1})\log(\text{MAP})- \text{MAT}\log(\text{MAP})) \\
&= \beta_1 + \beta_3\log(\text{MAP}) 
\end{align*}
```

```{=latex}
\begin{align*}
\frac{\text{ANPP}_2}{\text{ANPP}_1} &= e^{\beta_1 + \beta_3 \log\text{MAP}} \\
&= e^{\beta_1}*e^{\beta_3\log\text{MAP}} \\
&= e^{\beta_1} \exp(\beta_3)^\text{MAP}
\end{align*}
```

Similarly, note that the first term here is identical to the transformed coefficient for MAT. Let's calculate the second part:

```{r}
mat_base <- exp(b3)
mat_base
```

### Plotting marginal effects

```{r}
mat_vals <- seq(min(anpp$MAT), max(anpp$MAT), length.out = 100)
map_vals <- seq(min(anpp$MAP), max(anpp$MAP), length.out = 100)
marg_df <- data.frame(map_vals = map_vals,
                      mat_marginal = mat_base^map_vals,
                      mat_vals = mat_vals,
                      map_marginal = map_coeff*map_base^mat_vals)
mat_marg <- ggplot(data = marg_df,
                   aes(x = map_vals, y = mat_marginal))+
  geom_line()+
  labs(x = 'MAP Value', y = 'MAT|MAP')

map_marg <- ggplot(data = marg_df,
                   aes(x = mat_vals, y = map_marginal))+
  geom_line()+
  labs(x = 'MAT Value', y = 'MAP|MAT')

mat_marg+map_marg+
  plot_annotation(tag_levels = 'a', 
                              tag_prefix = '(', 
                              tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
```


# Figures

## Figure 1

Let's construct figure 1. The left panel (a) will be the ANPP histogram. The right panels (b, c) will be the scatterplots.

```{r}
fig1 <- anpp_hist+mat_scatter/map_scatter+
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig1
```

## Figure 2

The histograms for the predictors, side-by-side.
```{r}
fig2 <- mat_hist+map_hist+logmap_hist+
  plot_layout(axes = 'collect')+
  plot_annotation(tag_levels = 'a',
                  tag_prefix = '(',
                  tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig2
```

## Figure 3

These will be the residuals diagnostics plots.

```{r}
rd_df <- data.frame(predicted_values = predict(m1_log, anpp),
                    residuals = residuals(m1_log, type = 'pearson'),
                    stand_dev_residuals = rstandard(m1_log, type = 'deviance'),
                    stand_pear_residuals = rstandard(m1_log, type = 'pearson'),
                    hat_values = hatvalues(m1_log))

# get theoretical quantiles that match what base R uses
rd_df$theo_quant <- abs(qnorm(ppoints(nrow(rd_df))/2))

resids_fitted <- ggplot(rd_df, aes(x = predicted_values, y = residuals))+
  geom_point(alpha = 0.5)+
  geom_hline(aes(yintercept = 0), col = 'grey', lty = 'dashed')+
  geom_smooth(se = FALSE, col = 'red')+
  labs(x = 'Predicted Value',
       y = 'Pearson\'s Residual')

# Q-Q plot
qq <- ggplot(rd_df, aes(x = sort(theo_quant), y = sort(abs(stand_dev_residuals))))+
  geom_point(alpha = 0.5)+
  geom_abline(aes(intercept = 0, slope = 1), col = 'grey', lty = 'dashed')+
  labs(x = 'Theoretical Quantiles', y = '|Std. Deviance Residuals|')

scale_location <- ggplot(rd_df, aes(x = predicted_values, 
                                    y = sqrt(abs(stand_pear_residuals))))+
  geom_point(alpha = 0.5)+
  geom_smooth(se = FALSE, col = 'red')+
  labs(x = 'Predicted Value',
       y = expression(sqrt('|Std. Pearson\'s Residual|')))

resid_leverage <- ggplot(rd_df, aes(x = hat_values,
                                    y = stand_pear_residuals))+
  geom_point(alpha = 0.5)+
  geom_smooth(se = FALSE, col = 'red')+
  geom_hline(aes(yintercept = 0), col = 'grey', lty = 'dashed')+
  labs(x = 'Leverage',
       y = 'Std. Pearson\'s Residual')

fig3 <- (resids_fitted+qq)/(scale_location+resid_leverage)+
  plot_annotation(tag_levels = 'a',
                  tag_prefix = '(',
                  tag_suffix = ')')&
  theme(plot.tag = element_text(face = 'bold'))
fig3
```

