---
title: "Assign3"
format: html
---

# Setup

Some packages:
```{r}
#| label: packages
#| collapse: TRUE
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
set_theme(theme_bw()) # for ggplot formatting
```

Now the data:
```{r}
#| label: load-data
#| collapse: TRUE
anpp <- read.csv('global_ANPP_data.csv', header = TRUE) |>
  select(-X) # get rid of "X" column that's an artifact of the file
str(anpp)
```

AF only has 3 observations; if we include region as a factor, we should exclude these data because there just aren't enough to make robust inference.

# EDA: variable distributions -- Q1 and 2

## Histograms

> note for Noah: the #| lines at the beginning of chunks aren't R code -- they're options for Quarto in knitting and evaluating chunks. #| label: xxxx means label this chunk xxxx for organization/future reference.

A histogram plotting function to avoid a ton of redundant code:

```{r}
#| label: plot-hist
# We use {{var}} so we can use dplyr-style data masking: from Hadley Wickham
#> "programming with dplyr" page

plot_hist <- function(data, 
                      var,
                      color = 'grey20',
                      fill = 'lightgrey',
                      fill_alpha = 0.75,
                      num_bins = 10,
                      add_mean = TRUE,
                      line_col = 'red',
                      line_type = 'dashed'){
  p <- ggplot(data)+
    geom_histogram(aes(x = {{var}}),
                   color = color,
                   fill = fill,
                   bins = num_bins)+
      labs(y = 'Count')
  

  if(add_mean == TRUE){
    p <- p+geom_vline(aes(xintercept = mean({{var}})),
                      color = line_col,
                      linetype = line_type)
  }   
  
  return(p)
}
```


And now let's use the code to make some histograms.

```{r}
#| label: histograms
#| 

# MAT
mat_hist <- plot_hist(data = anpp,
                      var = MAT)

# MAP
map_hist <- plot_hist(data = anpp,
                      var = MAP)


# ANPP
anpp_hist <- plot_hist(data = anpp,
                       var = ANPP)

# Now make a figure:
## Overall
mat_hist+map_hist+anpp_hist+plot_layout(axis_titles = 'collect')

```


## Scatterplots

Again, a function to save typing. Since we're going to be using a gamma distribution with a log link, let's examine linearity of predictors WRT log(ANPP) (though this behavior is customizable with the `yvar` option).

```{r}
plot_scatter <- function(data,
                         xvar,
                         yvar = log(ANPP),
                         pt_alpha = 0.5,
                         smooths = TRUE,
                         smooth_col = 'blue',
                         smooth_lty = 'dashed',
                         smooth_lty_rgn = 'solid'){
  
  # plot template
  p <- ggplot(data, aes(x = {{xvar}}, y = {{yvar}}))+
    geom_point(alpha = pt_alpha)
  
  if(smooths == TRUE){
    p <- p+geom_smooth(formula = y ~ x,
                method = 'loess',
                se = FALSE,
                col = smooth_col,
                linetype = smooth_lty)
  }
  return(p)
}
```

Using that function:

```{r}
#| label: scatterplots

# MAT
mat_scatter <- plot_scatter(anpp, xvar = MAT, smooth_lty = 'solid')


# MAP
map_scatter <- plot_scatter(anpp, xvar = MAP, smooth_lty = 'solid')



# Log MAP
log_map_scatter <- plot_scatter(anpp, xvar = log(MAP), smooth_lty = 'solid')


# Figures
## Overall
map_scatter+mat_scatter+plot_layout(axis_titles = 'collect') 


## Log vs untransformed
map_scatter+log_map_scatter+plot_layout(axis_titles = 'collect')

```


## Takeaways (Q3)

We can see a few things here:

1. ANPP is positive, continuous, and skewed (from the histogram)
2. ANPP variance is not constant with respect to MAT (from the scatterplot): higher MAT has higher variance (heteroskedacity)
3. ANPP is not linearly related to MAP.
4. MAP is not normally distributed; it is strictly positive and right-skewed. log(MAP) is much closer to normal.
5. MAT is closer to normally distributed, but it still has a sort of weird shape.
6. We recover a more linear relationship between log(MAP) and ANPP, but variance is still non-constant.

Taking 1 and 2 together, we should use either a Gamma or Inverse Gaussian distribution as the response distribution.

# Approach to modeling

We want to know:

1. Which distribution? Gamma or inverse gaussian?
2. Which combination of predictors?
  - Include interactions?
  - Transform predictors?
 
Gamma vs Inverse Gaussian: 
https://link.springer.com/chapter/10.1007/978-1-4419-0118-7_11 (found a pdf)
- Gamma has variance related to mean^2
- Inverse gaussian has variance related to mean^3
- We can figure this out by plotting log(variance) against log(means) and seeing whether the slope is closer to 2 or 3.
- Typically use inverse gaussian with really really severe skew, which I don't think we have. There's some code in the graveyard formalizing this, but Gamma is our best bet I think.

Because Gamma is best bet, we can just look at (2). I think we can start with a saturated model and look at leverage. Based on that, transform predictors and see if that helps with leverage. Then start paring down the model.



From the plots, we see that the residuals vs leverage plot and the scale-location plot actually look worse than the original full model.


# Modeling 

## Full Model

As always, we'll start by running a full model.
 
### Running the model

```{r}
m1 <- glm(ANPP ~ MAT*MAP,
          family = Gamma(link = 'log'),
          data = anpp)
```
### Diagnostics

```{r}
plot(m1)
```

We have some higher leverage points (but still within acceptable Cook's distance); let's see where these points fall on the distribution of MAT and MAP:

```{r}
#| label: leverage-points

m1_lev <- c(37, 38, 46)
m1_mat_lev <- anpp[m1_lev, 3]
m1_map_lev <- anpp[m1_lev, 4]

hist(anpp$MAT)
abline(v = m1_mat_lev, col = 'red')

hist(anpp$MAP)
abline(v = m1_map_lev, col = 'red')
```

We can see that, in fact, the high leverage points for MAP are on the tail end. So then we can log-transform the MAP data to get it better behaved:

```{r}
#| label: examine-leverage
hist(log(anpp$MAP))
abline(v = log(m1_map_lev), col = 'red')
```


While these points still fall on the tails, they may have less leverage now. Let's check:


```{r}
#| label: full-model-log
m1_log <- glm(ANPP ~ MAT*log(MAP),
                family = Gamma(link = 'log'),
                data = anpp)
plot(m1_log)
```

Leverage plot looks a lot better, but residuals are a bit wonky at the tail end of the CDF (Q-Q plot). Let's see if this model fits better than the un-logged one, using AIC:

```{r}
AIC(m1, m1_log)
```

Looks like, as we suspected from residuals, that m1_log is better.

### ANOVA

Let's now look at an analysis of deviance to see if we can pare this model down.

```{r}
anova(m1_log)
```

It looks like no: both terms and their interactions contribute significantly to decreasing the residual deviance. Because these models are nested, we don't even need to use AIC. But just to check, let's run a model that doesn't include the interaction term.

## Simplified Model 2

### Running the model

Let's remove the interaction term. If all is right in the world, our ANOVA and AIC results

```{r}
m2_log <- glm(ANPP ~ log(MAP)+MAT,
              family = Gamma(link = 'log'),
              data = anpp)
```

### Diagnostics

```{r}
plot(m2_log)
```

Residuals vs fitted looks a little funky here. Let's use AIC to compare this model to the full model. If all is right in the world, this will have a higher AIC (agreeing with the ANOVA results):

```{r}
AIC(m1_log, m2_log, m1) |>
  arrange(AIC)
```

These results are as expected.

# Coefficients and interpretation

Now that we have selected our best-fitting model, let's look at the coefficients:

```{r}
summary(m1_log)
```

