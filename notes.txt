
# Predictor Transformations, from Dave

Recall that in a GLM, we are modelling 

$$
 g(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +...
$$
That is, we model a function of the mean as a linear combination of the predictors. That means that when we transform a predictor, we are saying that $g(\mu)$ is a function of the *transformation* of that predictor. 

Example:

Gamma distribution with log link. Log link means we are modeling:

$$
\log(\mu) = \beta_0 + \beta_1 x_1 + \beta_2 x_2+...
$$

Let's just take the simplest case with one predictor:

$$
\log(\mu) = \beta_0 + \beta_1 x
$$

Then log-transforming x would mean that we expect the relationship between $\log(\mu)$ and $\log(x)$ to be linear.

In a context with multiple predictor variables, including a transformed term can be:

- Because we have prior reason to do so, e.g., we know that volume is cubically related to radius
- Because we need to "pull" the curvature of the fitted term in some way.

In the second case, we figure this out by first fitting a model with untransformed predictors, then looking at the distance and leverage plots. If there are high-leverage points that are on the tail of a skewed predictor, we can consider transforming the predictor to lessen the skew and therefore reduce the leverage of those points.

This whole thing looks a lot like just log transforming the response...but the key difference is that, by using a Gamma distribution, we allow both the mean AND the variance to vary as a function of the predictors. Dave drove a picture on the board that had log(response) vs predictor, and the idea is that you expect a right-skewed distribution in log(response) values at each predictor value (as opposed to a normal distribution in log(response) at each predictor value), AND that you expect those right-skewed distributions to change shape with increasing predictor values.

# Graveyard 

Since MAP is not super well behaved, let's try some transformations. Since MAP is positive and non-zero, we could try log or square root transforms. 

```{r}
#| label: graveyard1
#| eval: FALSE
#| echo: FALSE

# log transform map
logmap_hist <- ggplot(anpp, aes(x = log(MAP)))+
    geom_histogram(color = 'black',
                 fill = 'lightgrey',
                 bins = num_bins)+
  geom_vline(aes(xintercept = mean(log(MAP))),
             linetype = 'dashed',
             color = 'red')+
  labs(y = 'Count')

# square root transform map
sqrtmap_hist <- ggplot(anpp, aes(x = sqrt(MAP)))+
    geom_histogram(color = 'black',
                 fill = 'lightgrey',
                 bins = num_bins)+
  geom_vline(aes(xintercept = mean(sqrt(MAP))),
             linetype = 'dashed',
             color = 'red')+
  labs(y = 'Count')

logmap_hist+sqrtmap_hist
```

Clearly log does a better job of dealing with the weird distribution of MAP data. I'm not sure there's much to do about MAT because it contains negative values (ruling out square root) and zero (ruling out log). A reciprocal transform (1/MAT) would just push all the values toward the interval (0, 1) and not necessarily help. We could try a polynomial transform...but I don't think that is well supported either theoretically or in the scatterplot shown below. Also, using an even power will give us a bimodal distribution because negative and positive values with the same absolute value get mapped to the same squared (or ^4, ^6...) value.

## Gamma vs inv gaus
Let's look at log(variance) vs log(means). We'll approximate this by grouping MAP and MAT and calculating variance and mean for each group.

```{r}
#| label: variance-mean

# group MAP and MAT into 4 groups (number picked somewhat arbitrarily)
anpp$matg <- cut(anpp$MAT, breaks = 4)
anpp$mapg <- cut(anpp$MAP, breaks = 4)

# get variances for each MAT group/MAP group combination
var <- tapply(anpp$ANPP, list(anpp$matg, anpp$mapg), FUN = 'var')
means <- tapply(anpp$ANPP, list(anpp$matg, anpp$mapg), FUN = 'mean')

# plot
plot(log(var), log(means))

lm(c(log(var)) ~ c(log(means)))

```
